{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf794de8-d8a9-4f79-a625-789921780103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q -U bitsandbytes transformers peft accelerate datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85a26d42-e443-4556-b03a-e209fb475fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, BitsAndBytesConfig\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model, PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c067c7b-0050-420c-b576-6943e9346409",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/tokenization_auto.py:757: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_name = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name,\n",
    "    padding_side=\"left\",\n",
    "    add_eos_token=True,\n",
    "    add_bos_token=True,)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "#tokenizer.padding_side = \"right\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c4393d6-3c30-40a5-9e93-77442b97d2f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py:468: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a94077d8566e44cf96cd234cfd55c84e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, use_auth_token = \"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\",\n",
    "                                             quantization_config=bnb_config,\n",
    "                                             torch_dtype=torch.float16,\n",
    "                                             device_map=\"auto\",\n",
    "                                            # attn_implementation=\"flash_attention_2\",   #You can use flash attention on your local GPU with specific libraries\n",
    "                                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87630e53-5ea7-4b38-a270-1995ae1f3e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('\"A cat wearing a bow tie and playing the piano\",', '\"This cat thinks it\\'s a maestro, but it\\'s really just making a \\'meow\\'sic mess!\",'), ('\"A dog surfing on a wave\",', '\"When the waves are calling, even dogs answer! Hang ten, pupper!\",'), ('\"A group of rabbits having a picnic\",', '\"These rabbits know how to do lunch right! But watch out for carrot crumbs!\",'), ('\"A bear riding a unicycle\",', '\"Life\\'s a balancing act, just like this bear on a unicycle!\",'), ('\"A horse wearing a crown and being pampered by servants\",', '\"Who knew horses had royal aspirations? This one\\'s living its best fairy tale!\",')]\n"
     ]
    }
   ],
   "source": [
    "def read_caption_phrase_pairs(captions_file, phrases_file):\n",
    "    caption_phrase_pairs = []\n",
    "\n",
    "    with open(captions_file, 'r') as f_captions, open(phrases_file, 'r') as f_phrases:\n",
    "        for caption_line, phrase_line in zip(f_captions, f_phrases):\n",
    "            caption = caption_line.strip().replace('\\\\', '')\n",
    "            phrase = phrase_line.strip().replace('\\\\', '')\n",
    "            caption_phrase_pairs.append((caption, phrase))\n",
    "\n",
    "        # Ensure the number of captions and phrases match\n",
    "        if len(caption_phrase_pairs) == 0 or len(caption_phrase_pairs[0]) != 2:\n",
    "            raise ValueError(\"Number of captions and phrases do not match.\")\n",
    "\n",
    "    return caption_phrase_pairs\n",
    "\n",
    "# Example usage:\n",
    "captions_file = \"captions.txt\"\n",
    "phrases_file = \"phrases.txt\"\n",
    "pairs = read_caption_phrase_pairs(captions_file, phrases_file)\n",
    "print(pairs[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45d8bd30-5c56-42f8-87aa-29507066378c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130\n",
      "101\n"
     ]
    }
   ],
   "source": [
    "print(len(pairs))\n",
    "print(max(map(lambda x: len(x[1]), pairs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "134c693e-b26a-4d30-a1ac-e8e707366ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_eval_prompt(p):\n",
    "    sys_msg= \"Create a funny phrase/meme on \"\n",
    "    p =  \"[INST]\" + sys_msg +\"\\n\"+ p + \"[/INST]\"\n",
    "    return p "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4786a8ab-e240-48ab-8f70-031f8169c0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(user_query):\n",
    "    sys_msg= \"Create a funny phrase/meme on \"\n",
    "    p =  \"[INST]\" + sys_msg +\"\\n\"+ user_query['caption'] + \"[/INST]\" +  user_query['phrase']\n",
    "    return p "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8cc496f-c7e9-4d44-840b-90833222711f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare model for k-bit training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "#tokenizer.pad_token = \"!\"\n",
    "CUTOFF_LEN = 256\n",
    "LORA_R = 8\n",
    "LORA_ALPHA = 2 * LORA_R\n",
    "LORA_DROPOUT = 0.1\n",
    "\n",
    "\n",
    "config = LoraConfig(r=LORA_R, lora_alpha=LORA_ALPHA, target_modules=[ \"w1\", \"w2\", \"w3\"], lora_dropout=LORA_DROPOUT, bias=\"none\", task_type=\"CAUSAL_LM\")\n",
    "model = get_peft_model(model, config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f8ada11-6d45-4f54-8162-60ee5fd318d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize = lambda prompt: tokenizer(prompt, truncation=True, max_length=CUTOFF_LEN, padding=\"max_length\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "551fb3c0-093f-4211-ae1f-bd69f4e6f455",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def create_dataloader(qa_pairs, batch_size=8, shuffle=True):\n",
    "    # Create a dataset from the qa_pairs\n",
    "    dataset = Dataset.from_dict({'caption': [pair[0] for pair in qa_pairs],\n",
    "                                 'phrase': [pair[1] for pair in qa_pairs]})\n",
    "\n",
    "    # Create a dataset dictionary\n",
    "    dataset_dict = DatasetDict({'train': dataset})\n",
    "\n",
    "    # Create a dataloader from the dataset\n",
    "    dataloader = DataLoader(dataset_dict['train'],\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=shuffle)\n",
    "    return dataloader, dataset_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2cb7f5e-c58e-47d4-afcb-fa83f63a30e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader, ds = create_dataloader(pairs, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "43e30039-699a-4e36-831b-d088cb51608a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = ds[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d64118f6-cf27-4a5a-9c7f-4e3cc09d9181",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['caption', 'phrase'],\n",
       "    num_rows: 130\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b35e7c5d-f368-413e-8c9b-bec68a6aec0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f3079cbd3d24c589841fb69a19f0089",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/130 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data = train_data.shuffle().map(lambda x: tokenize(generate_prompt(x)), remove_columns=[\"caption\" , \"phrase\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "15883533-a107-479d-8db2-ab3fd5ab978c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask'],\n",
       "    num_rows: 130\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3ac63722-4355-4dbf-a1fd-f30407dec1fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataset=train_data,\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=4,\n",
    "        num_train_epochs=3,\n",
    "        learning_rate=1e-4,\n",
    "        logging_steps=2,\n",
    "        optim=\"adamw_torch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        output_dir=\"mixtral-moe-lora-instruct-shapeskeare\"\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    ")\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "afb61598-b9a2-409b-88a8-6241ca9b4dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='96' max='96' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [96/96 26:58, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>8.249000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5.599200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>4.096400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.727600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.264400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>2.253300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.815100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.703200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.546800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.554100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>1.511400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.386200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>1.382700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>1.294900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.163800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>1.204900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>1.151300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>1.048000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>1.110000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.097300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>1.026500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>1.145200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>1.023100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>1.133100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.974800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.964600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>1.116700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>1.050600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>1.094600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.910700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.940800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>1.106700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>1.042000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.959000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.975900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.999900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.906100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.956300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.866800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.851400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>0.922100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>1.059600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>1.045400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>1.141200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.113800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>1.079000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>1.158800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>1.057000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/peft/utils/other.py:581: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-66228a1a-0419f6a46b1e8c266046c4e9;a2ee24ff-83b0-4fb5-9245-d6db18309c18)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1/resolve/main/config.json.\n",
      "Repo model mistralai/Mixtral-8x7B-Instruct-v0.1 is gated. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in mistralai/Mixtral-8x7B-Instruct-v0.1.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in mistralai/Mixtral-8x7B-Instruct-v0.1 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/other.py:581: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-66228c43-11da2b18192762c71aac003d;b4c08856-d618-4760-8ab8-b1ac4363d8e2)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1/resolve/main/config.json.\n",
      "Repo model mistralai/Mixtral-8x7B-Instruct-v0.1 is gated. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in mistralai/Mixtral-8x7B-Instruct-v0.1.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in mistralai/Mixtral-8x7B-Instruct-v0.1 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/other.py:581: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-66228e54-0007a44c30216b632d7aed44;2185d0c2-a493-4335-a038-f34c34907371)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1/resolve/main/config.json.\n",
      "Repo model mistralai/Mixtral-8x7B-Instruct-v0.1 is gated. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in mistralai/Mixtral-8x7B-Instruct-v0.1.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in mistralai/Mixtral-8x7B-Instruct-v0.1 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=96, training_loss=1.5162874932090442, metrics={'train_runtime': 1635.8703, 'train_samples_per_second': 0.238, 'train_steps_per_second': 0.059, 'total_flos': 2.753591392390349e+16, 'train_loss': 1.5162874932090442, 'epoch': 2.953846153846154})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2b85cac7-0577-4f6b-ac20-6d6497c6ea7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.use_cache = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b6299fee-1ecd-49d3-b2a9-f5cce7557a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "\n",
    "image_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
    "image_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f2a767fa-ed25-4a7b-a0e8-38a1cff1c9dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "several people are sitting around a table with a large blueprint\n"
     ]
    }
   ],
   "source": [
    "img_path = \"6.JPEG\"\n",
    "raw_image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "# unconditional image captioning\n",
    "inputs = image_processor(raw_image, return_tensors=\"pt\")\n",
    "\n",
    "out = image_model.generate(**inputs)\n",
    "\n",
    "output_text = image_processor.decode(out[0], skip_special_tokens=True)\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b1601c5c-9934-45ca-9318-f1195814139f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> [INST]Create a funny phrase/meme on \n",
      "several people are sitting around a table with a large blueprint[/INST]</s>\"This group's got it all: brains, brawn, and big blueprints!\", \"Blueprint buddies: where every idea is a masterpiece in the making!\", \"Building bonds one blueprint at a time!\". These phrases capture the fun and collaborative spirit of this group working together to bring their ideas to life. The use of words like 'brains', 'brawn', and 'masterpiece' add humor by highlighting the group's intelligence\n"
     ]
    }
   ],
   "source": [
    "eval_prompt = generate_eval_prompt(output_text)\n",
    "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=100, repetition_penalty=1.15)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64aef271-cdf4-4761-b8e5-6c060183ec89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
